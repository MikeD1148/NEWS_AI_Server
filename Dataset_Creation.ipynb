{"cells":[{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1681304436675,"user":{"displayName":"Michael W","userId":"12711211480391300162"},"user_tz":-60},"id":"sOfIqPzutzvL"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from keras.preprocessing.text import Tokenizer\n","from googletrans import Translator\n","import csv\n","import time"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1858,"status":"ok","timestamp":1681292721107,"user":{"displayName":"Michael W","userId":"12711211480391300162"},"user_tz":-60},"id":"gus9WTwMuGYo","outputId":"62b3008b-00ef-4d4d-eeb9-86bd5f0129dc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Connect Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":224,"status":"ok","timestamp":1681300326290,"user":{"displayName":"Michael W","userId":"12711211480391300162"},"user_tz":-60},"id":"pnnDAhjm3Vu4"},"outputs":[],"source":["# Load Dataset\n","old_rows = pd.read_csv('/content/drive/MyDrive/small_dataset.csv', header=None, names=['text', 'category']).values\n","new_rows = []"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":682921,"status":"ok","timestamp":1681305246377,"user":{"displayName":"Michael W","userId":"12711211480391300162"},"user_tz":-60},"id":"JfoNGekyuNut","outputId":"f9c5325e-657c-4c77-88bb-fd5b09611c90"},"outputs":[],"source":["def data_augmentation(text, native='en', foreign=None):\n","  # Initialise translator\n","  translator = Translator()\n","\n","  # Translate to foreign language\n","  translated_text = translator.translate(text, src=native, dest=foreign).text\n","\n","  # Translate back to English\n","  augmented = translator.translate(translated_text, src=foreign, dest=native).text\n","\n","  print(\"article done\" + \"\\n\")\n","  return augmented\n","\n","# Open file to store augmented dataset\n","with open('/content/drive/MyDrive/large_dataset.csv', 'w', newline='') as file:\n","  writer = csv.writer(file)\n","\n","  # Augment each row of the original dataset\n","  for row in old_rows:\n","    # Translate can't parse very long text\n","    truncated_text = row[0][:1000]\n","    # Repeating for 4 languages to vastly increase dataset size\n","    writer.writerow([row[0], row[1]])\n","    new_data_spanish = data_augmentation(truncated_text, foreign='es')\n","    writer.writerow([new_data_spanish, row[1]])\n","    new_data_french = data_augmentation(truncated_text, foreign='fr')\n","    writer.writerow([new_data_french, row[1]])\n","    new_data_german = data_augmentation(truncated_text, foreign='de')\n","    writer.writerow([new_data_german, row[1]])\n","    new_data_afrikaans = data_augmentation(truncated_text, foreign='af')\n","    writer.writerow([new_data_afrikaans, row[1]])\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMhzkF2dQxtyMF5btuxfrnD","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
